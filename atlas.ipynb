{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtEa763EFE5HjlZqt7lgX/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanialPahlavan/RAG/blob/main/atlas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prepare Files"
      ],
      "metadata": {
        "id": "UFD3yp3orrAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Clone project"
      ],
      "metadata": {
        "id": "niznqJtlqDkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sk6s8EHF-Uue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174f839d-eab5-41f7-b18f-742235594f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'atlas'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 144 (delta 35), reused 13 (delta 10), pack-reused 89 (from 1)\u001b[K\n",
            "Receiving objects: 100% (144/144), 162.21 KiB | 13.52 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/atlas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd atlas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S27-sN_3d7t0",
        "outputId": "5dd1b60c-ab07-4e05-f995-fab31d895fdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/atlas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Install Pre Requirments\n",
        "\n",
        "may needs restart runtime for prepare old libs"
      ],
      "metadata": {
        "id": "-TvjvKGys3vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALUzTvj5d_Au",
        "outputId": "ef319257-6449-4226-bb13-567b658a48c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m785.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2024.7.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True if the GPU is being used\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd9DE-Fxe4H3",
        "outputId": "7d865abd-cce3-46a3-98f5-1aab6e4b8799"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu==1.7.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI9LupyMekSS",
        "outputId": "96f2e95f-96af-4fae-bb9e-c072704a69ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu==1.7.2\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvph8eGEemSi",
        "outputId": "0874d5e3-e7ed-4fa9-996e-e9d6c2ffabbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairscale==0.4.6 (from -r requirements.txt (line 1))\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.18.0 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge==1.0.1 (from -r requirements.txt (line 3))\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.17.0)\n",
            "Collecting wget (from -r requirements.txt (line 5))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from fairscale==0.4.6->-r requirements.txt (line 1)) (1.11.0+cu113)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.18.0->-r requirements.txt (line 2))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0->-r requirements.txt (line 2)) (4.66.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.1->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (71.0.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0->-r requirements.txt (line 2)) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0->-r requirements.txt (line 2)) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0->-r requirements.txt (line 2)) (1.4.2)\n",
            "Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale, wget\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307221 sha256=53c8f8d9a2e1b324aec3bf1dcabd5bb3aacfce578ead2fa7124193bf8627debd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=fccacfedcc259ecb31fe202c9b6c867135a25428d0af8e4e04c9223f575b2859\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built fairscale wget\n",
            "Installing collected packages: wget, tokenizers, sacremoses, rouge, fairscale, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed fairscale-0.4.6 rouge-1.0.1 sacremoses-0.1.1 tokenizers-0.12.1 transformers-4.18.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download preprocess and datasets"
      ],
      "metadata": {
        "id": "Cz4OgiUv_I0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 download Corpora\n",
        "\n",
        "\n",
        "\n",
        "| Corpus Name                               | Corpus Download Key                                 | Description                                                                    | Size |\n",
        "|--------------------------------------------|----------------------------------------------------|--------------------------------------------------------------------------------|-------|\n",
        "| enwiki-dec2017                               | `corpora/wiki/enwiki-dec2017`                        | Wikipedia dump from Dec 2017, preprocessed into passages                         | 30.4M (26.9M text, 2.7M infobox) |\n",
        "| enwiki-dec2018                               | `corpora/wiki/enwiki-dec2018`                        | Wikipedia dump from Dec 2018, preprocessed into passages (recommended for NQ, TriviaQA) | 32.1M (28.4M text, 3.7M infobox) |\n",
        "| enwiki-aug2019                               | `corpora/wiki/enwiki-aug2019`                        | Wikipedia dump from August 2019, preprocessed into passages                         | 33.1M (29.4M text, 3.8M infobox) |\n",
        "| enwiki-dec2020                               | `corpora/wiki/enwiki-dec2020`                        | Wikipedia dump from Dec 2020, preprocessed into passages                         | 35.6M (31.5M text, 4.1M infobox) |\n",
        "| enwiki-dec2021                               | `corpora/wiki/enwiki-dec2021`                        | Wikipedia dump from Dec 2021, preprocessed into passages                         | 37.5M (33.1M text, 4.3M infobox) |\n",
        "\n",
        "\n",
        "aware: \"\"We cannot open-source the common-crawl indices used in the paper at this time.\"\" source : github repo of this project\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyQV-y28rB-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "W2MiLaslDQJc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The corpus that was used to train the model can be downloaded with :\n",
        "#The $DATA_DIR folder is used to store everything we download.\n",
        "\n",
        "#!python preprocessing/download_corpus.py --corpus {corpus download key} --output_directory ${DATA_DIR}\n",
        "#if you assigned DAT_DIR use below:\n",
        "#!python preprocessing/download_corpus.py --corpus corpora/wiki/enwiki-dec2018 --output_directory $DATA_DIR\n",
        "!python preprocessing/download_corpus.py --corpus corpora/wiki/enwiki-dec2018 --output_directory /content/atlas/data"
      ],
      "metadata": {
        "id": "NuCMg79erEvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f640ea45-86a3-44e2-8aed-5a8c28ef6209"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/atlas/corpora/wiki/enwiki-dec2018/text-list-100-sec.jsonl to /content/atlas/data/corpora/wiki/enwiki-dec2018/text-list-100-sec.jsonl\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/corpora/wiki/enwiki-dec2018/infobox.jsonl to /content/atlas/data/corpora/wiki/enwiki-dec2018/infobox.jsonl\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 models\n",
        "\n",
        "We are open-sourcing pretrained Atlas models at base, large, xl and xxl sizes. These include both the pretrained retriever and reader weights. In addition, we're open-sourcing our strongest-performing fully-finetuned NaturalQuestions Atlas models, for users who want to perform state-of-the-art QA inference (or finetune them on other QA tasks). Models can be downloaded as follows:\n",
        "\n",
        "\n",
        "| Model | Model Download Key | Description | Parameters (reader / retriever) |\n",
        "|---|---|---|---|\n",
        "| Atlas-xxl | `models/atlas/xxl` | Pretrained Atlas XXL model | 11B / 110M |\n",
        "| Atlas-xl | `models/atlas/xl` | Pretrained Atlas XL model | 3B / 110M |\n",
        "| Atlas-large | `models/atlas/large` | Pretrained Atlas large model | 770M / 110M |\n",
        "| Atlas-base | `models/atlas/base` | Pretrained Atlas base model | 220M / 110M |\n",
        "| NQ-finetuned Atlas-xxl | `models/atlas_nq/xxl` | Atlas XXL model, finetuned on Natural Question | 11B / 110M |\n",
        "| NQ-finetuned Atlas-xl | `models/atlas_nq/xl` | Atlas XL model, finetuned on Natural Question | 3B / 110M |\n",
        "| NQ-finetuned Atlas-large | `models/atlas_nq/large` | Atlas large model, finetuned on Natural Question | 770M / 110M |\n",
        "| NQ-finetuned Atlas-base | `models/atlas_nq/base` | Atlas base model, finetuned on Natural Question | 220M / 110M |\n",
        "\n"
      ],
      "metadata": {
        "id": "6VRFGVIRr1jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the model checkpoints:\n",
        "\n",
        "#python preprocessing/download_model.py --model {model download key} --output_directory ${DATA_DIR}\n",
        "\n",
        "!python preprocessing/download_model.py --model models/atlas/xl --output_directory /content/atlas/data"
      ],
      "metadata": {
        "id": "owmYvBG1r4C5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5318b378-d971-4282-c2b0-3baa8a630520"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/atlas/models/atlas/xl/model.pth.tar to /content/atlas/data/models/atlas/xl/model.pth.tar\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Pre-built Indices\n",
        "Atlas will automatically build an index if none is provided. This is convenient, but can take a long time, especially with fewer GPU workers, or if the index is very large.\n",
        "\n",
        "We have therefore made precomputed indices available for download for the wiki-dec2018 corpus for the pretrained Atlas checkpoints, and for nq-finetuned Atlas checkpoints\n",
        "\n",
        "These can be downloaded as follows :\n",
        "\n",
        "\n",
        "| Index | Index Download Key | Corresponding Model | Description |\n",
        "|---|---|---|---|\n",
        "| Atlas XXL wiki-dec2018 index | `indices/atlas/wiki/xxl` | `models/atlas/xxl` | Precomputed index for the wiki-dec2018 corpus for the pretrained Atlas-xxl model |\n",
        "| Atlas XL wiki-dec2018 index | `indices/atlas/wiki/xl` | `models/atlas/xl` | Precomputed index for the wiki-dec2018 corpus for the pretrained Atlas-xl model |\n",
        "| Atlas large wiki-dec2018 index | `indices/atlas/wiki/large` | `models/atlas/large` | Precomputed index for the wiki-dec2018 corpus for the pretrained Atlas-large model |\n",
        "| Atlas base wiki-dec2018 index | `indices/atlas/wiki/base` | `models/atlas/base` | Precomputed index for the wiki-dec2018 corpus for the pretrained Atlas-base model |\n",
        "| Atlas-nq XXL wiki-dec2018 index | `indices/atlas_nq/wiki/xxl` | `models/atlas_nq/xxl` | Precomputed index for the wiki-dec2018 corpus for the natural-questions-finetuned Atlas xxl model |\n",
        "| Atlas-nq XL wiki-dec2018 index | `indices/atlas_nq/wiki/xl` | `models/atlas/xl` | Precomputed index for the wiki-dec2018 corpus for the natural-questions-finetuned Atlas xl model |\n",
        "| Atlas-nq large wiki-dec2018 index | `indices/atlas_nq/wiki/large` | `models/atlas/large` | Precomputed index for the wiki-dec2018 corpus for the natural-questions-finetuned Atlas large model |\n",
        "| Atlas-nq base wiki-dec2018 index | `indices/atlas_nq/wiki/base` | `models/atlas/base` | Precomputed index for the wiki-dec2018 corpus for the natural-questions-finetuned Atlas base model |\n",
        "\n"
      ],
      "metadata": {
        "id": "gqiDlIG2sHuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the prebuilt indexes:\n",
        "\n",
        "#python preprocessing/download_index.py --index {index download key} --output_directory ${DATA_DIR}\n",
        "!python preprocessing/download_index.py --index indices/atlas/wiki/xl --output_directory /content/atlas/data"
      ],
      "metadata": {
        "id": "Xml4S6Tusanr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6266238-a8d9-4345-d676-4eb8d6297d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.0.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.0.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.0.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.0.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.1.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.1.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.1.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.1.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.2.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.2.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.2.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.2.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.3.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.3.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.3.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.3.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.4.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.4.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.4.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.4.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.5.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.5.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/embeddings.5.pt to /content/atlas/data/indices/atlas/wiki/xl/embeddings.5.pt\n",
            "\n",
            "Downloading https://dl.fbaipublicfiles.com/atlas/indices/atlas/wiki/xl/passages.6.pt to /content/atlas/data/indices/atlas/wiki/xl/passages.6.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, to download the NQ data:\n",
        "\n",
        "!python preprocessing/prepare_qa.py --output_directory /content/atlas/data"
      ],
      "metadata": {
        "id": "hV-CHW6L6dmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Finetuning - [needs a machine with 8 GPUs (A100 with 80 GB of RAM) ]\n",
        "\n"
      ],
      "metadata": {
        "id": "dFimwVPi7m7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 make own datasets\n",
        "\n",
        "make question and asnwer with this pattern and save jsonl file in this place\n",
        "\n",
        "$DATA_DIR/nq_data/train.100-shot.jsonl\n",
        "\n",
        "pattern:\n",
        "\n",
        "**{\"question\": \"when is season 2 of punisher coming out on netflix\", \"answers\": [\"in 2019\"]}**\n",
        "\n",
        "\n",
        "\n",
        "we create a folder to collect the experiments output and the run logs:"
      ],
      "metadata": {
        "id": "zco-QEZ181He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $DATA_DIR/experiments"
      ],
      "metadata": {
        "id": "BVmkJfCc79En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 finetune model\n",
        "We now pass all the paths where the models and data were downloaded to the torchrun command. In a machine with 8 GPUs (A100 with 80 GB of RAM), we can run the following to conduct the finetuning:\n",
        "\n"
      ],
      "metadata": {
        "id": "Sf_p3JnN8BzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchrun --standalone --nnodes 1 --nproc_per_node 8  finetune_qa.py --train_data\n",
        "$DATA_DIR/nq_data/train.100-shot.jsonl --eval_data $DATA_DIR/nq_data/test.jsonl --name\n",
        "\"my_finetuning_experiment_baseline_xl\"  --checkpoint_dir $DATA_DIR/experiments/ --total_steps 31\n",
        "--index_mode flat --model_path $DATA_DIR/models/atlas/xl --load_index_path\n",
        "$DATA_DIR/indices/atlas/wiki/xl --reader_model_type  google/t5-xl-lm-adapt"
      ],
      "metadata": {
        "id": "PjtQiaMN8DO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 finetune model - alternative\n",
        "Alternatively, we can conduct the finetuning using a product quantized index, with light compression corresponding to a code size of 192. In order to use a Faiss product-quantized index, we select --index_mode faiss and --faiss_index_type pq respectively and compression parameter --faiss_code_size 192:.\n",
        "\n"
      ],
      "metadata": {
        "id": "EWSYfUad8Sw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchrun --standalone --nnodes 1 --nproc_per_node 8  finetune_qa.py --train_data\n",
        "$DATA_DIR/nq_data/train.100-shot.jsonl --eval_data $DATA_DIR/nq_data/test.jsonl --name\n",
        "\"my_finetuning_experiment_pq_192_xl\"  --checkpoint_dir $DATA_DIR/experiments/ --total_steps 31\n",
        "--index_mode faiss --faiss_index_type  pq --faiss_code_size 192 --model_path\n",
        "$DATA_DIR/models/atlas/xl --load_index_path $DATA_DIR/indices/atlas/wiki/xl --reader_model_type\n",
        "google/t5-xl-lm-adapt"
      ],
      "metadata": {
        "id": "jUPjzrvY8UWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 finetune model - alternative\n",
        "\n",
        "We can also use a more aggressive compression parameter — corresponding to a code size of 64 — and conduct the finetuning with the product quantized index using only 2 GPUs:\n",
        "\n"
      ],
      "metadata": {
        "id": "0VRkDOQ18VdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchrun --standalone --nnodes 1 --nproc_per_node 2  finetune_qa.py --train_data\n",
        "$DATA_DIR/nq_data/train.100-shot.jsonl --eval_data $DATA_DIR/nq_data/test.jsonl --name\n",
        "\"my_finetuning_experiment_pq_64_xl\"  --checkpoint_dir $DATA_DIR/experiments/ --total_steps 31\n",
        "--index_mode faiss --faiss_index_type  pq --faiss_code_size 64 --model_path $DATA_DIR/models/atlas/xl\n",
        "--load_index_path $DATA_DIR/indices/atlas/wiki/xl --reader_model_type  google/t5-xl-lm-adapt"
      ],
      "metadata": {
        "id": "Q4fYXQOG8WoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## others - Dump info -need clean it\n"
      ],
      "metadata": {
        "id": "y9oskirpsb3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "\n",
        "Atlas can train (or evaluate) on any supervised learning task which can be formulated in a \"seq2seq\" format, where there is a sequence of 1 or more tokens comprising an input query and a sequence of 1 or more tokens comprising an output target. For example, a query might be a question, Where is the Bermuda Triangle?, and a target might be the answer to that question, Western part of the North Atlantic Ocean. This way of modelling will be familiar to users of models like T5 or BART. Anywhere these models could be used, Atlas can be used too, using the exact same data: Atlas will learn to retrieve passages from its retrieval index by itself - annotations for associating passages to (query, target) pairs are not used.\n",
        "\n",
        "The Atlas codebase configures what task it is doing, and what evaluation metrics to call using the --task command line argument. We have implemented a base task, with only the most basic support for seq2seq training, but provide more fully-featured functionality for Masked Language Modelling (mlm), Language Modelling (lm), Wikipedia section generation (section), Open-domain QA (QA), Multiple choice QA (multiple_choice), fact checking (fever), and the KILT suite (kilt), All tasks expect input data formatted as jsonl format, but the specific field names are task specific. Some tasks have additional command line args, and specialized evaluation. Adding new tasks is straightforward, and described here.\n",
        "\n",
        "The tasks are described in more detail below, and most have example commands in examples/{task}/ .\n",
        "\n",
        "Base Task\n",
        "Masked Language Modelling\n",
        "Language Modelling\n",
        "Wikipedia Section Generation\n",
        "Open-Domain Question Answering (e.g. NaturalQuestions, TriviaQA, TempLama)\n",
        "Multiple Choice Question Answering (e.g. MMLU)\n",
        "FEVER Fact Verification\n",
        "KILT"
      ],
      "metadata": {
        "id": "sdK06UDd7blN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Base Task\n",
        "\n",
        "This is the most basic task available, and is probably not the best option for you, especially if your task closely resembles one the other implemented tasks.\n",
        "\n",
        "Specify this task by passing --task base to either train.py or evaluate.py\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py or evaluate.py space-separated lists to --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. This task expects input files to have a query field with the input query string and a target field with the output query string, e.g.:\n",
        "\n",
        "{\"query\": \"input to Atlas\", \"target\": \"desired generation from Atlas\"}\n",
        "\n",
        "The evaluation loop will calculate evaluation loss and the fraction of eval data examples where Atlas generates an output that exactly matches the target. If you pass --write_results to the script, Atlas predictions on the eval data will be written to the save checkpoint directory with the following format:\n",
        "\n",
        "\n",
        "{\"query\": \"input to Atlas\", \"answers\": [\"desired generation from Atlas\"], \"generation\": \"Atlas's prediction for the query\", \"passages\": [\"list of retrieved passages\"]}"
      ],
      "metadata": {
        "id": "MF2axagts_2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Masked Language Modelling\n",
        "The Masked Language modelling task implements the Masked Language Modelling pretraining task as introduced by T5. This is the task we use to pretrain the main Atlas in the paper.\n",
        "\n",
        "Specify this task by passing --task mlm to train.py.\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. These files should be comprised of JSON objects with the following format:\n",
        "\n",
        "{\n",
        "  \"text\": \"text passage to apply noise to and train to de-noise\",\n",
        "  \"id\": \"unique id of text passage\"\n",
        "  ... # you can have other fields you want to keep around for ease of analysis, but they wont actually be used\n",
        "}\n",
        "\n",
        "\n",
        "The intention is that the same files that you use for the retrieval corpus, (passed to --passages) can be used as training data. The task will apply the T5 noise function to text field, to automatically create inputs and target generations.\n",
        "\n",
        "The MLM task will prevent Atlas from retrieving the passage that it is trying to de-noise. It does this by filtering out any passage from retrieved results which have same id field as the instance Atlas is de-noising. This functionality is important if the de-noising training data and the passages Atlas is retrieving from are the same corpus.\n",
        "\n",
        "This task has the following task specific args:\n",
        "\n",
        "  --mlm_noise_density MLM_NOISE_DENSITY\n",
        "      how much of an input text should be masked by masking spans (default: 0.15)\n",
        "  --mlm_mean_noise_span_length MLM_MEAN_NOISE_SPAN_LENGTH\n",
        "      average length of an MLM masking span (default: 3)\n",
        "  --min_words_per_lm_instance MIN_WORDS_PER_LM_INSTANCE\n",
        "      Instances with fewer than min_words_per_lm_instance instances will be skipped for MLM/LM/Section generation (default: None)\n",
        "\n",
        "If you pass --write_results, Atlas will write its mask-filling predictions to file.\n",
        "\n",
        "Atlas will log the following evaluation metrics for MLM during its evaluation loop:\n",
        "\n",
        "eval_loss: evaluation reader loss of generated mlm mask-fill spans\n",
        "accuracy: fraction of perfectly de-noised mask-fill spans\n",
        "f1: token f1 fraction of correct de-noised mask-fill spans\n",
        "rouge_1: rouge 1 score of generated mask-fill spans relative to the gold reference masked spans\n",
        "rouge_2: rouge 2 score of generated mask-fill spans relative to the gold reference masked spans\n",
        "rouge_L: rouge L score of generated mask-fill spans relative to the gold reference masked spans\n",
        "\n"
      ],
      "metadata": {
        "id": "zPWjbBbLtLyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Modelling\n",
        "\n",
        "Atlas can be trained to do Left-to-Right Language Modeling by passing --task lm to train.py.\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. These files should be comprised of JSON objects with the following format:\n",
        "\n",
        "{\n",
        "  \"text\": \"text passage to train Atlas to generate\",\n",
        "  \"id\": \"unique id of text passage\"\n",
        "  ... # you can have other fields you want to keep around for ease of analysis, but they wont actually be used\n",
        "}\n",
        "\n",
        "The intention is that the same files that you use for the retrieval corpus, (passed to --passages) can be used as training data. The task will preprocess the text field automatically, dividing it into two random segments - the left part serves as conditioning context, and the right part is the text the Atlas model will be trained to generate as a continuation.\n",
        "\n",
        "The LM task will prevent Atlas from retrieving the same passage that it is trying to generate. It does this by filtering out any passage from retrieved results which have same id field as the instance Atlas is generating. This functionality is important if the de-noising training data and the passages Atlas is retrieving from are the same corpus.\n",
        "\n",
        "This task has the following task specific args:\n",
        "\n",
        "  --min_words_per_lm_instance MIN_WORDS_PER_LM_INSTANCE\n",
        "      Instances with fewer than min_words_per_lm_instance instances will be skipped for  MLM/LM/Section generation (default: None)\n",
        "  --min_lm_context_ratio MIN_LM_CONTEXT_RATIO\n",
        "      Splits text into two segments for language modelling.' 'Left segment is conditioning context, right segment is for generating.' 'The left segment must be more than min_lm_context_ratio of\n",
        "      the right segment (default: 0.5)\n",
        "  --max_lm_context_ratio MAX_LM_CONTEXT_RATIO\n",
        "      Splits text into two segments for language modelling.' 'Left segment is conditioning context, right segment is for generating.' 'The left segment must be less than max_lm_context_ratio\n",
        "      of the right segment (default: 0.5)\n",
        "\n",
        "If you pass --write_results, Atlas will write its lm predictions to file.\n",
        "\n",
        "Atlas will log the following evaluation metrics for LM during its evaluation loop:\n",
        "\n",
        "eval_loss: evaluation reader loss of continuations for the reference data\n",
        "accuracy: fraction of perfectly predicted continuations\n",
        "f1: token f1 fraction of correct generated continuations\n",
        "rouge_1: rouge 1 score of generated continuations relative to the gold reference continuations\n",
        "rouge_2: rouge 2 score of generated continuations relative to the gold reference continuations\n",
        "rouge_L: rouge L score of generated continuations relative to the gold reference continuations"
      ],
      "metadata": {
        "id": "d0k245-ttaIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia Section Generation\n",
        "\n",
        "Atlas can be trained to generate the text of a wikipedia passage given its title and section title, by passing --task section to train.py.\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should have the form of the text-list-100-sec.jsonl files in the wikipedia dumps. These can be obtained by following the instructions in Available Data and Models for download, for example the training file: enwiki-dec2018/text-list-100-sec.jsonl. These files should be comprised of JSON objects, one per line, with the following format:\n",
        "\n",
        "{\n",
        "  \"id\": \"3793043\",\n",
        "  \"title\": \"Bermuda Triangle\",\n",
        "  \"section\": \"Compass variations\",\n",
        "  \"text\": \" Compass problems are one of the cited phrases in many Triangle incidents. While some have theorized that unusual local magnetic anomalies may exist in the area, such anomalies have not been found. Compasses have natural magnetic variations in relation to the magnetic poles, a fact which navigators have known for centuries.\"\n",
        "}\n",
        "\n",
        "\n",
        "The task will automatically format the input query to the model as \"{Title}, {Section}\" - e.g. in this example, the input to Atlas will be constructed as Bermuda Triangle, Compass Variations. The output will be the text field of the example. The section task will prevent Atlas from retrieving the same passage that it is trying to generate. It does this by filtering out any passage from retrieved results which have same id field as the instance Atlas is generating.\n",
        "\n",
        "This task has the following task specific args:\n",
        "\n",
        "  --min_words_per_lm_instance MIN_WORDS_PER_LM_INSTANCE\n",
        "      Instances with fewer than min_words_per_lm_instance instances will be skipped for MLM/LM/Section generation (default: None)\n",
        "\n",
        "If you pass --write_results, Atlas will write its generated predictions for the text for Wikipedia sections to file.\n",
        "\n",
        "Atlas will log the following evaluation metrics for section during its evaluation loop:\n",
        "\n",
        "eval_loss: evaluation reader loss of continuations for the reference data\n",
        "accuracy: fraction of perfectly predicted continuations\n",
        "f1: token f1 fraction of correct generated continuations\n",
        "rouge_1: rouge 1 score of generated continuations relative to the gold reference continuations\n",
        "rouge_2: rouge 2 score of generated continuations relative to the gold reference continuations\n",
        "rouge_L: rouge L score of generated continuations relative to the gold reference continuations"
      ],
      "metadata": {
        "id": "wiHxarg8ti3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open-Domain Question Answering (e.g. NaturalQuestions, TriviaQA, TempLama)\n",
        "\n",
        "Atlas can be trained to answer open-domain QA questions by passing --task qa to train.py or evaluate.py. There is a worked example of QA in the Getting Started and Codebase at a Glance section. We use this task for the NaturalQuestions, TriviaQA and TempLama datasets in the paper.\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. Files should have one JSON instance per line with the following format:\n",
        "\n",
        "{\n",
        "  \"question\": \"where is the bermuda triangle\",\n",
        "  \"answers\": [\"Western part of the North Atlantic Ocean\"],\n",
        "   ... # you can have other fields you want to keep around for ease of analysis, but they wont actually be used\n",
        "}\n",
        "\n",
        "The question will be formatted according to the task specific argument --qa_prompt_format, which defaults to question: {question} answer: <extra_id_0>. For example above, the question would be automatically formatted as input queries to Atlas as question: where is the bermuda triangle answer: <extra_id_0>. The supervision target is obtained from the target field. If this field does not exist, the supervision target will get selected at random from the available answers in the answers field, and formatted as <extra_id_0> {answer}.\n",
        "\n",
        "If you pass --write_results, Atlas will write its predicted answers to file.\n",
        "\n",
        "Atlas will log the following evaluation metrics for open domain QA during its evaluation loop:\n",
        "\n",
        "eval_loss: evaluation reader loss of evaluation answers.\n",
        "exact_match: Open-domain QA exact match score of generated answers\n",
        "f1: Open-domain QA F1 score of generated answers\n",
        "Natural Questions & TriviaQA\n",
        "You can download the NaturalQuestions and TriviaQA data by calling:\n",
        "\n",
        "python preprocessing/prepare_qa.py --output_directory ${DATA_DIR}\n",
        "\n",
        "which will download train.jsonl, train.64-shot.jsonl (the fewshot training dataset we use), dev.jsonl and test.jsonl to ${DATA_DIR}/data/nq_data and ${DATA_DIR}/data/triviaqa_data.\n",
        "\n",
        "Example scripts for running fewshot and standard finetuning and evaluation with a wikipedia index for NQ can be found in examples/nq. This script can be used for TriviaQA by swapping the train/dev/test files.\n",
        "\n",
        "TempLama\n",
        "We defined a cloze-question answering task for assessing index faithfulness and temporal transfer, derived from the TempLAMA dataset.\n",
        "\n",
        "You can download the TempLAMA data and create and format our derived dataset by calling the following script:\n",
        "\n",
        "python preprocessing/prepare_templama.py --output_directory ${DATA_DIR}\n",
        "\n",
        "which will create the files temp_lama.train.2017.jsonl, temp_lama.valid.2017.jsonl, temp_lama.test.2017.jsonl, temp_lama.train.2020.jsonl, temp_lama.valid.2020.jsonl, temp_lama.test.2020.jsonl under ${DATA_DIR}/data/templama_data/. These files will contain cloze questions, with answers specific to that year.\n",
        "\n",
        "Example scripts for running training and evaluation for TempLama can be found at examples/templama. (note the use of qa_prompt_format {question}, which switches off the automatic QA prompt formatting used for TriviaQA and NQ)\n"
      ],
      "metadata": {
        "id": "1lr4CC-wtrZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Choice Question Answering (e.g. MMLU)\n",
        "\n",
        "Atlas can be trained to answer multiple choice questions by passing --task multiple_choice to train.py or evaluate.py. We use this task for our experiments with MMLU.\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. Files should have one JSON instance per line with the following format:\n",
        "\n",
        "{\n",
        "  \"question\": \"Which of the following is the body cavity that contains the pituitary gland?\",\n",
        "  \"options\": {\n",
        "    \"A\": \"Abdominal\",\n",
        "    \"B\": \"Cranial\",\n",
        "    \"C\": \"Pleural\",\n",
        "    \"D\": \"Spinal\"\n",
        "    ... # you can have more (or fewer) answer options as long as they have alphabetically consecutive upper case letter keys, starting at A\n",
        "  },\n",
        "  \"answer\": \"B\",\n",
        "  ... # you can have other fields you want to keep around for ease of analysis, but they wont actually be used\n",
        "}\n",
        "\n",
        "These will get automatically formatted into input queries for Atlas of the form question: {question} answers: (A) {options['A']} (B) {options['B']} (C) {options['C']} (D) {options['D']} Answer: <extra_id_0>, with target generations of the format <extra_id_0> {answer letter}. The example above would get formatted to: question: {Which of the following is the body cavity that contains the pituitary gland? answers: (A) Abdominal (B) Cranial (C) Pleural (D) Spinal Answer: <extra_id_0>, with the target generation {extra_id_0} B.\n",
        "\n",
        "Multiple-Choice QA has the following task specific args:\n",
        "\n",
        "  --multiple_choice_num_options\n",
        "      How many choice options for multiple choice QA (MMLU is 4) (default: 4)\n",
        "  --multiple_choice_train_permutations {single,cyclic,all}\n",
        "      Whether to train with answer order permutations When training on multiple choice (e.g. MMLU). Can improve results by de-biasing models's preferences for arbitrary answer orderings. Recommend\n",
        "      training with 'all'. single: no permutations. cyclic: cyclic permutations. all: all possible answer order permutations' (default: single)\n",
        "  --multiple_choice_eval_permutations {single,cyclic,all}\n",
        "      Whether to evaluate with answer order permutations for multiple choice (e.g. MMLU). Can improve results by de-biasing models's preferences for arbitrary answer orderings. Best results with\n",
        "      'all' but very slow. 'cyclic' is a good compromise. single: no permutations. cyclic: cyclic permutations. all: all possible answer order permutations' (default: single)\n",
        "\n",
        "The permutation options will automatically duplicate the inputs, but with the answer orders permuted (e.g. With \"A\" now being \"cranial\", \"B\" being \"pleural\" etc.) This improves results for when we have very small amounts of supervised data (or zeroshot). The code will automatically marginalize across results for evaluation permutations for you, in the case you use --multiple_choice_eval_permutations option cyclic or all. More details on the permutation de-biasing can be found in the appendix of Atlas: Few-shot Learning with Retrieval Augmented Language Models.\n",
        "\n",
        "If you pass --write_results, Atlas will write its predicted answers to file, with the following format:\n",
        "\n",
        "{\n",
        "  \"question\": \"the prompt-template applied input\",\n",
        "  \"generation\": \"answer letter choice with highest probability after marginalizing across permutations\",\n",
        "  \"choice_probs\": \"the probability of each answer choice (normalized over total answer options)\",\n",
        "  \"all_probs\": \"the un-marginalized answer probabilities from all the answer order permutations\",\n",
        "  \"permutations\": [\"the list of prediction objects for each permutation of the answer ordering\"]\n",
        "}\n",
        "\n",
        "MMLU\n",
        "A dedicated ReadMe is available for running MMLU experiments here. There is a tool to download and preprocess the MMLU data, and example scripts for running each of the experimental settings that we explore with MMLU are available examples/mmlu. These are documented in detail in the MMLU Dedicated Readme.\n",
        "\n"
      ],
      "metadata": {
        "id": "vWtHNgR7t1dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FEVER Fact Verification\n",
        "\n",
        "Atlas can be trained to classify textual claims as \"SUPPORTED\", \"REFUTED\" or \"NOT_ENOUGH_INFO\" by a corpus, such as for the FEVER task by using --task fever to train.py or evaluate.py.\n",
        "\n",
        "You can download the FEVER data by calling the following script:\n",
        "\n",
        "python preprocessing/prepare_fever.py --output_directory ${DATA_DIR}\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. Files should have one JSON instance per line with the following format:\n",
        "\n",
        "{\n",
        "  \"claim\": \"the claim to assess\",\n",
        "  \"label\": \"either 'SUPPORTS', 'REFUTES' or 'NOT ENOUGH INFO'\",\n",
        "   ... # you can have other fields you want to keep around for ease of analysis, but they wont actually be used\n",
        "}\n",
        "\n",
        "\n",
        "Atlas will automatically process these instances, and format them for input as question: {claim} answer: <extra_id_0> and the output as <extra_id_0> {true, false or maybe}. If you pass --write_results, Atlas will write its predicted labels to file. Atlas will log the following evaluation metrics for open domain QA during its evaluation loop:\n",
        "\n",
        "accuracy: how many claims were correctly classified by the model."
      ],
      "metadata": {
        "id": "Cf1pAN98uDNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KILT\n",
        "\n",
        "\n",
        "Atlas can be trained to perform KILT tasks by using --task kilt to train.py or evaluate.py.\n",
        "\n",
        "KILT data can be obtained from here\n",
        "\n",
        "Train/validation/test data for this task should consist of jsonl files, which should be passed to train.py as --train_data train_file_1.jsonl train_file_2.jsonl, and --eval_data eval_file_1.jsonl eval_file_2.jsonl etc. Files should have one JSON instance per line with the following format (i.e. the codebase will accept the KILT format directly)\n",
        "\n",
        "{'id': # original data point id if available otherwise unique id\n",
        " 'input': # question / claim / sentence / etc\n",
        " 'output': [ # each element might contain an answer, a provenance or both\n",
        "    {\n",
        "    'answer': # answer in textual form\n",
        "    'provenance': [\n",
        "        # evidence set for the answer from the KILT ks\n",
        "        {\n",
        "            'wikipedia_id':  # *mandatory*\n",
        "            'title':\n",
        "            'section':\n",
        "            'start_paragraph_id':\n",
        "            'start_character':\n",
        "            'end_paragraph_id':\n",
        "            'end_character':\n",
        "            'bleu_score': # wrt original evidence\n",
        "            'meta': # dataset/task specific\n",
        "        }\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        " 'meta': # dataset/task specific\n",
        " }\n",
        "\n",
        "Atlas will automatically process these instances appropriately, into Atlas] query inputs based on the input field and target generations based on the answer fields\n",
        "\n",
        "If you pass --write_results, Atlas will write its predicted labels to file.\n",
        "\n",
        "Atlas will log the following evaluation metrics for open domain QA during its evaluation loop:\n",
        "\n",
        "accuracy: how often generations exactly match the reference\n",
        "exact_match: how often generations exactly match the reference, with open-domain QA normalization applied\n",
        "f1: the token level f1 score overlap between the generation and reference\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgGjh8XOuOnE"
      }
    }
  ]
}