{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1AJHI_7Eb6B"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOC66pLqTKSSZibGmy/0wRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanialPahlavan/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# intro\n",
        "Basic RAG from article Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
        "\n",
        "https://arxiv.org/abs/2005.11401"
      ],
      "metadata": {
        "id": "gGEfteUC4g_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoRC from THU-KEG\n",
        "\n",
        "https://github.com/THU-KEG/KoRC/tree/main"
      ],
      "metadata": {
        "id": "bYao2zmo5DOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/THU-KEG/KoRC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwhAtkXi49UL",
        "outputId": "70d2bbd2-4b2d-4b49-d8dd-3bc92b76573f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoRC'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 132 (delta 49), reused 119 (delta 41), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (132/132), 85.80 KiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCYP6N4d4U9d",
        "outputId": "2aa508cd-e4ec-4a1f-ac39-453502267162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-18 14:00:49--  https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.108, 3.163.189.14, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4694541059 (4.4G) [application/gzip]\n",
            "Saving to: ‘psgs_w100.tsv.gz’\n",
            "\n",
            "psgs_w100.tsv.gz    100%[===================>]   4.37G   100MB/s    in 23s     \n",
            "\n",
            "2024-05-18 14:01:12 (191 MB/s) - ‘psgs_w100.tsv.gz’ saved [4694541059/4694541059]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d psgs_w100.tsv.gz"
      ],
      "metadata": {
        "id": "KCTne-4K5zZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z0K40wD8JR4",
        "outputId": "6848ec51-b70f-4c82-c9e7-4beb826bbbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocess"
      ],
      "metadata": {
        "id": "MrSogmECFS4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### small DB"
      ],
      "metadata": {
        "id": "UnGgMQXSEWXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Open the file with a context manager to ensure it's closed properly\n",
        "with open('psgs_w100.tsv', 'r') as file:\n",
        "    new_lines = []\n",
        "    # Use enumerate for counting lines and tqdm for progress bar\n",
        "    for i, line in enumerate(tqdm(file), start=1):\n",
        "        # Skip the first line (header)\n",
        "        if i == 1:\n",
        "            continue\n",
        "        # Break the loop if 500 lines have been processed\n",
        "        if i > 500:\n",
        "            break\n",
        "        id, text, title = line.strip().split(\"\\t\")\n",
        "        new_lines.append(title + '\\t' + text + '\\n')\n",
        "\n",
        "# Specify the full path to the file\n",
        "with open('/content/KoRC/RAG/preprocess/data.csv', 'w') as f:\n",
        "    f.writelines(new_lines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3TeYT1oEbJ4",
        "outputId": "07e563f0-bbba-4eb2-8195-6d2d2faccc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [00:00, 111248.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full DB\n",
        "just use small or full not both"
      ],
      "metadata": {
        "id": "E1AJHI_7Eb6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "new_lines = []\n",
        "for line in tqdm(open('psgs_w100.tsv').readlines()[1:]):\n",
        "    id, text, title = line.strip().split(\"\\t\")\n",
        "    new_lines.append(title+'\\t'+text+'\\n')\n",
        "\n",
        "# Specify the full path to the file\n",
        "with open('/content/KoRC/RAG/preprocess/data.csv', 'w') as f:\n",
        "    f.writelines(new_lines)\n"
      ],
      "metadata": {
        "id": "W-foX5guDqyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/preprocess/convert.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJkDlSf257gX",
        "outputId": "cd8371c5-6dfc-46bc-ab9f-b97760dd6890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build_faiss"
      ],
      "metadata": {
        "id": "gV4ZiLOEFcvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCQ50dW9YD7",
        "outputId": "4984325e-157d-4d64-cda9-7ae0b813ead3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libomp-dev\n",
        "!python -m pip install --upgrade faiss-gpu\n",
        "!python -m pip install --upgrade faiss-cpu\n",
        "\n",
        "#gpu\n",
        "#!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayvSWMnk9vit",
        "outputId": "dcf83f9c-a50e-4f82-8adc-70365595e78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (1:14.0-55~exp2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/preprocess/build_faiss.py --csv_path /content/KoRC/RAG/preprocess/data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCmtuuNt-itZ",
        "outputId": "d4979b85-f474-4863-e79e-ad8e8d7dc54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Step 1 - Create the dataset\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
            "Saving the dataset (1/1 shards): 100% 499/499 [00:00<00:00, 36890.71 examples/s]\n",
            "INFO:__main__:Step 2 - Index the dataset\n",
            "100% 1/1 [00:00<00:00, 10.23it/s]\n",
            "INFO:__main__:Step 3 - Load RAG\n",
            "config.json: 100% 4.60k/4.60k [00:00<00:00, 14.3MB/s]\n",
            "(…)_encoder_tokenizer/tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 287kB/s]\n",
            "question_encoder_tokenizer/vocab.txt: 100% 232k/232k [00:00<00:00, 8.47MB/s]\n",
            "(…)ncoder_tokenizer/special_tokens_map.json: 100% 112/112 [00:00<00:00, 570kB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "(…)enerator_tokenizer/tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 160kB/s]\n",
            "generator_tokenizer/vocab.json: 100% 899k/899k [00:00<00:00, 4.94MB/s]\n",
            "generator_tokenizer/merges.txt: 100% 456k/456k [00:00<00:00, 2.45MB/s]\n",
            "(…)erator_tokenizer/special_tokens_map.json: 100% 772/772 [00:00<00:00, 4.69MB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "pytorch_model.bin: 100% 2.06G/2.06G [00:09<00:00, 207MB/s]\n",
            "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "INFO:__main__:Step 4 - Have fun\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "INFO:__main__:Q: What does Moses' rod turn into ?\n",
            "INFO:__main__:A:  a snake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare_open_dataset.py"
      ],
      "metadata": {
        "id": "NWrP8afoHK4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/prepare_openqa_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epaqpCC4HMpZ",
        "outputId": "35c9525a-1dd8-49c1-d5ad-a03b260d8e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/KoRC/RAG/prepare_openqa_dataset.py\", line 35, in <module>\n",
            "    mrc_train = json.load(open(os.path.join(input_path,'train.json')))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 76, in join\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_feature.py"
      ],
      "metadata": {
        "id": "YtNuWu9GHQ20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash seq.human.sh"
      ],
      "metadata": {
        "id": "yEmhDjm_HZTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}