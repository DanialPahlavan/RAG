{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1AJHI_7Eb6B"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdGVr/fIpAmz8/cwDt278t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanialPahlavan/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# intro\n",
        "Basic RAG from article Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
        "\n",
        "https://arxiv.org/abs/2005.11401"
      ],
      "metadata": {
        "id": "gGEfteUC4g_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoRC from THU-KEG\n",
        "\n",
        "https://github.com/THU-KEG/KoRC/tree/main"
      ],
      "metadata": {
        "id": "bYao2zmo5DOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/THU-KEG/KoRC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwhAtkXi49UL",
        "outputId": "91821f59-afba-4c6f-c77a-78049a31f7e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoRC'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 132 (delta 49), reused 119 (delta 41), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (132/132), 85.80 KiB | 1.13 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCYP6N4d4U9d",
        "outputId": "1861b29c-53b7-48a8-c0db-44885daa88a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 12:50:05--  https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.128, 13.35.7.38, 13.35.7.82, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4694541059 (4.4G) [application/gzip]\n",
            "Saving to: ‘psgs_w100.tsv.gz’\n",
            "\n",
            "psgs_w100.tsv.gz    100%[===================>]   4.37G  80.5MB/s    in 41s     \n",
            "\n",
            "2024-08-14 12:50:46 (109 MB/s) - ‘psgs_w100.tsv.gz’ saved [4694541059/4694541059]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d psgs_w100.tsv.gz"
      ],
      "metadata": {
        "id": "KCTne-4K5zZc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z0K40wD8JR4",
        "outputId": "7300726f-0695-4126-ef77-699d7da5bb0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocess"
      ],
      "metadata": {
        "id": "MrSogmECFS4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### small DB"
      ],
      "metadata": {
        "id": "UnGgMQXSEWXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Open the file with a context manager to ensure it's closed properly\n",
        "with open('psgs_w100.tsv', 'r') as file:\n",
        "    new_lines = []\n",
        "    # Use enumerate for counting lines and tqdm for progress bar\n",
        "    for i, line in enumerate(tqdm(file), start=1):\n",
        "        # Skip the first line (header)\n",
        "        if i == 1:\n",
        "            continue\n",
        "        # Break the loop if 500 lines have been processed\n",
        "        if i > 500:\n",
        "            break\n",
        "        id, text, title = line.strip().split(\"\\t\")\n",
        "        new_lines.append(title + '\\t' + text + '\\n')\n",
        "\n",
        "# Specify the full path to the file\n",
        "with open('/content/KoRC/RAG/preprocess/data.csv', 'w') as f:\n",
        "    f.writelines(new_lines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3TeYT1oEbJ4",
        "outputId": "29ef38de-071a-40c1-c97a-534b3924bfe0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [00:00, 29964.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full DB\n",
        "just use small or full not both"
      ],
      "metadata": {
        "id": "E1AJHI_7Eb6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "new_lines = []\n",
        "for line in tqdm(open('psgs_w100.tsv').readlines()[1:]):\n",
        "    id, text, title = line.strip().split(\"\\t\")\n",
        "    new_lines.append(title+'\\t'+text+'\\n')\n",
        "\n",
        "# Specify the full path to the file\n",
        "with open('/content/KoRC/RAG/preprocess/data.csv', 'w') as f:\n",
        "    f.writelines(new_lines)\n"
      ],
      "metadata": {
        "id": "W-foX5guDqyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/preprocess/convert.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJkDlSf257gX",
        "outputId": "cd8371c5-6dfc-46bc-ab9f-b97760dd6890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build_faiss"
      ],
      "metadata": {
        "id": "gV4ZiLOEFcvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    !pip install datasets\n",
        "    !apt-get install libomp-dev -y\n",
        "    !pip install --upgrade faiss-gpu faiss-cpu\n",
        "except Exception as e:\n",
        "    print(f\"Error installing dependencies: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCQ50dW9YD7",
        "outputId": "24e00d2e-a391-4722-9042-65b0dacf3950"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libomp-14-dev libomp5-14\n",
            "Suggested packages:\n",
            "  libomp-14-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-14-dev libomp-dev libomp5-14\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 738 kB of archives.\n",
            "After this operation, 8,991 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp5-14 amd64 1:14.0.0-1ubuntu1.1 [389 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp-14-dev amd64 1:14.0.0-1ubuntu1.1 [347 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libomp-dev amd64 1:14.0-55~exp2 [3,074 B]\n",
            "Fetched 738 kB in 2s (360 kB/s)\n",
            "Selecting previously unselected package libomp5-14:amd64.\n",
            "(Reading database ... 123594 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libomp-14-dev.\n",
            "Preparing to unpack .../libomp-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libomp-dev:amd64.\n",
            "Preparing to unpack .../libomp-dev_1%3a14.0-55~exp2_amd64.deb ...\n",
            "Unpacking libomp-dev:amd64 (1:14.0-55~exp2) ...\n",
            "Setting up libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libomp-dev:amd64 (1:14.0-55~exp2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1 faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libomp-dev\n",
        "!python -m pip install --upgrade faiss-gpu\n",
        "!python -m pip install --upgrade faiss-cpu\n",
        "\n",
        "#gpu\n",
        "#!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayvSWMnk9vit",
        "outputId": "5815f347-f86f-4b5e-8ff0-70cb9754455d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (1:14.0-55~exp2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/preprocess/build_faiss.py --csv_path /content/KoRC/RAG/preprocess/data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCmtuuNt-itZ",
        "outputId": "845017d1-3add-4444-a931-0c99062064bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Step 1 - Create the dataset\n",
            "Generating train split: 499 examples [00:00, 6543.11 examples/s]\n",
            "Map: 100% 499/499 [00:00<00:00, 32833.80 examples/s]\n",
            "config.json: 100% 492/492 [00:00<00:00, 2.15MB/s]\n",
            "pytorch_model.bin: 100% 438M/438M [00:20<00:00, 21.3MB/s]\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 125kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 670kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.34MB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
            "Map:   0% 0/499 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Map: 100% 499/499 [00:07<00:00, 67.37 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 499/499 [00:00<00:00, 54164.17 examples/s]\n",
            "INFO:__main__:Step 2 - Index the dataset\n",
            "100% 1/1 [00:00<00:00, 19.56it/s]\n",
            "INFO:__main__:Step 3 - Load RAG\n",
            "config.json: 100% 4.60k/4.60k [00:00<00:00, 15.8MB/s]\n",
            "(…)_encoder_tokenizer/tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 241kB/s]\n",
            "question_encoder_tokenizer/vocab.txt: 100% 232k/232k [00:00<00:00, 676kB/s]\n",
            "(…)ncoder_tokenizer/special_tokens_map.json: 100% 112/112 [00:00<00:00, 524kB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "(…)enerator_tokenizer/tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 126kB/s]\n",
            "generator_tokenizer/vocab.json: 100% 899k/899k [00:00<00:00, 1.33MB/s]\n",
            "generator_tokenizer/merges.txt: 100% 456k/456k [00:00<00:00, 905kB/s]\n",
            "(…)erator_tokenizer/special_tokens_map.json: 100% 772/772 [00:00<00:00, 3.57MB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "pytorch_model.bin: 100% 2.06G/2.06G [00:46<00:00, 44.5MB/s]\n",
            "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "INFO:__main__:Step 4 - Have fun\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "INFO:__main__:Q: What does Moses' rod turn into ?\n",
            "INFO:__main__:A:  a snake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare_open_dataset.py"
      ],
      "metadata": {
        "id": "NWrP8afoHK4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/KoRC/RAG/prepare_openqa_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epaqpCC4HMpZ",
        "outputId": "a0b0d23d-88ed-485a-94dc-ed4e6f8053f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/KoRC/RAG/prepare_openqa_dataset.py\", line 35, in <module>\n",
            "    mrc_train = json.load(open(os.path.join(input_path,'train.json')))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 76, in join\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.chdir('/content/KoRC/RAG/')\n",
        "    print(\"Changed to /content/KoRC/RAG/ directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error changing directory: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXVW-mlX4slF",
        "outputId": "51e2fa02-8b78-4a6e-8a25-b0a3ea4f764c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed to /content/KoRC/RAG/ directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_feature.py -o output_directory --max_tgt_len 128 --max_source_length 256 --max_context_length 512 --question_with_ctx"
      ],
      "metadata": {
        "id": "YtNuWu9GHQ20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f199111-c72b-488b-e77a-250d5a14aaad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add special tokens ['<ans>']\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 113kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 672kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 898kB/s]\n",
            "config.json: 100% 492/492 [00:00<00:00, 3.18MB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
            "pytorch_model.bin: 100% 438M/438M [00:20<00:00, 21.0MB/s]\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/KoRC/RAG/prepare_feature.py\", line 40, in <module>\n",
            "    ds = load_dataset(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2606, in load_dataset\n",
            "    builder_instance = load_dataset_builder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2277, in load_dataset_builder\n",
            "    dataset_module = dataset_module_factory(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 1814, in dataset_module_factory\n",
            "    ).get_module()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 1149, in get_module\n",
            "    data_files = DataFilesDict.from_patterns(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 721, in from_patterns\n",
            "    else DataFilesList.from_patterns(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 624, in from_patterns\n",
            "    resolve_pattern(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 411, in resolve_pattern\n",
            "    raise FileNotFoundError(error_msg)\n",
            "FileNotFoundError: Unable to find '/content/KoRC/RAG/output_directorytrain.jsonl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9O_KtMt-sWh",
        "outputId": "77103844-56da-48aa-85f4-c04f22ab6617"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoRC/RAG/script\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.chdir('/content/KoRC/RAG/')\n",
        "    print(\"Changed to /content/KoRC/RAG/ directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error changing directory: {e}\")\n",
        "\n",
        "try:\n",
        "    # Upgrade pytorch_lightning to ensure all required modules are available\n",
        "    os.system('pip install --upgrade pytorch-lightning')\n",
        "\n",
        "    # Ensure necessary directories exist\n",
        "    os.system('mkdir -p ../dataset/rag/human/seq1e-5/small_iid_result')\n",
        "    os.system('mkdir -p ../dataset/rag/human/seq1e-5/small_ood_result')\n",
        "\n",
        "    # Run the prepare_feature.py script with the required arguments\n",
        "    os.system('python prepare_feature.py -o output_directory --max_tgt_len 128 --max_source_length 256 --max_context_length 512 --question_with_ctx')\n",
        "\n",
        "    # Run the Bash script\n",
        "    os.system('bash seq.human.sh')\n",
        "except Exception as e:\n",
        "    print(f\"Error running final scripts: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjPy0P1p_C1j",
        "outputId": "f40eb12e-0bc1-4235-cbf1-6d2c82f25fa7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed to /content/KoRC/RAG/ directory.\n",
            "Error running command: Command 'python prepare_feature.py -o output_directory --max_tgt_len 128 --max_source_length 256 --max_context_length 512 --question_with_ctx' returned non-zero exit status 1.\n",
            "Output: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash seq.human.sh"
      ],
      "metadata": {
        "id": "yEmhDjm_HZTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47eb436-f01b-4fe1-cf9a-9cbb9bce13b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: seq.human.sh: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}